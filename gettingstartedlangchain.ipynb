{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic_AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000027726E1E360> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000277273917C0> root_client=<openai.OpenAI object at 0x000002772681DFD0> root_async_client=<openai.AsyncOpenAI object at 0x0000027727320350> model_name='o4-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o4-mini\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here‚Äôs a high‚Äêlevel overview of the three offerings from the LangChain ecosystem:\\n\\n1. LangChain  \\n   ‚Ä¢ What it is  \\n     ‚Äì An open-source SDK (Python, JavaScript/TypeScript) for building applications around large language models (LLMs).  \\n   ‚Ä¢ Core concepts  \\n     ‚Äì Prompts & prompt templates  \\n     ‚Äì Chains (pipelines of LLM calls, tools, retrievers, etc.)  \\n     ‚Äì Agents (LLM-powered ‚Äúreasoning‚Äù with tool use and action selection)  \\n     ‚Äì Memory (stateful conversations)  \\n     ‚Äì Data-connectors & retrieval (vectorstores, document loaders, etc.)  \\n   ‚Ä¢ Use cases  \\n     ‚Äì Chatbots and conversational agents  \\n     ‚Äì Retrieval-augmented generation (RAG) for Q&A  \\n     ‚Äì Summarization, translation, programmatic workflows\\n\\n2. LangGraph (sometimes called ‚Äúlangraph‚Äù)  \\n   ‚Ä¢ What it is  \\n     ‚Äì A small companion library to visualize/debug the structure of your LangChain chains and agent runs.  \\n   ‚Ä¢ Core functionality  \\n     ‚Äì Generates a call-graph (nodes = LLM/tool calls, edges = data flow)  \\n     ‚Äì Renders interactive diagrams (Graphviz, Mermaid, etc.) so you can inspect each step‚Äôs inputs/outputs  \\n   ‚Ä¢ Why use it  \\n     ‚Äì Quickly spot bottlenecks or unintended loops in your chain  \\n     ‚Äì Share visual documentation of your app‚Äôs logic\\n\\n3. LangSmith  \\n   ‚Ä¢ What it is  \\n     ‚Äì A hosted platform (by LangChain Labs) for logging, testing, evaluating, and deploying your LangChain-based apps.  \\n   ‚Ä¢ Key features  \\n     ‚Äì Call logging & replay: track every prompt, response, tool action  \\n     ‚Äì Prompt & model versioning: keep history of changes, A/B test prompts, providers  \\n     ‚Äì Evaluation suites: write ‚Äúunit tests‚Äù for your chains, track success/failure metrics  \\n     ‚Äì Visual tooling: chain/agent inspection, LangGraph integration, usage dashboards  \\n     ‚Äì Deployments: host agents as APIs, set up auth, usage quotas, monitoring  \\n   ‚Ä¢ Benefits  \\n     ‚Äì Accelerates development by giving you end-to-end observability  \\n     ‚Äì Helps maintain quality as your app and team scale\\n\\nIn short:  \\n‚Ä¢ LangChain = the local SDK for wiring together LLMs, tools, retrievers, memories.  \\n‚Ä¢ LangGraph/langraph = a visualization/debug tool for seeing how your LangChain chains run.  \\n‚Ä¢ LangSmith = the cloud service & UI for logging, testing, evaluating, and deploying those chains/agents in production.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1257, 'prompt_tokens': 17, 'total_tokens': 1274, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-BaqMw1Ao4uBa9bY7S7OMDRcKauEBW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--bdd342fe-895b-4f7f-946e-506d8c808b02-0' usage_metadata={'input_tokens': 17, 'output_tokens': 1257, 'total_tokens': 1274, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is langchain, langraph and langsmith?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here‚Äôs a high‚Äêlevel overview of the three offerings from the LangChain ecosystem:\n",
      "\n",
      "1. LangChain  \n",
      "   ‚Ä¢ What it is  \n",
      "     ‚Äì An open-source SDK (Python, JavaScript/TypeScript) for building applications around large language models (LLMs).  \n",
      "   ‚Ä¢ Core concepts  \n",
      "     ‚Äì Prompts & prompt templates  \n",
      "     ‚Äì Chains (pipelines of LLM calls, tools, retrievers, etc.)  \n",
      "     ‚Äì Agents (LLM-powered ‚Äúreasoning‚Äù with tool use and action selection)  \n",
      "     ‚Äì Memory (stateful conversations)  \n",
      "     ‚Äì Data-connectors & retrieval (vectorstores, document loaders, etc.)  \n",
      "   ‚Ä¢ Use cases  \n",
      "     ‚Äì Chatbots and conversational agents  \n",
      "     ‚Äì Retrieval-augmented generation (RAG) for Q&A  \n",
      "     ‚Äì Summarization, translation, programmatic workflows\n",
      "\n",
      "2. LangGraph (sometimes called ‚Äúlangraph‚Äù)  \n",
      "   ‚Ä¢ What it is  \n",
      "     ‚Äì A small companion library to visualize/debug the structure of your LangChain chains and agent runs.  \n",
      "   ‚Ä¢ Core functionality  \n",
      "     ‚Äì Generates a call-graph (nodes = LLM/tool calls, edges = data flow)  \n",
      "     ‚Äì Renders interactive diagrams (Graphviz, Mermaid, etc.) so you can inspect each step‚Äôs inputs/outputs  \n",
      "   ‚Ä¢ Why use it  \n",
      "     ‚Äì Quickly spot bottlenecks or unintended loops in your chain  \n",
      "     ‚Äì Share visual documentation of your app‚Äôs logic\n",
      "\n",
      "3. LangSmith  \n",
      "   ‚Ä¢ What it is  \n",
      "     ‚Äì A hosted platform (by LangChain Labs) for logging, testing, evaluating, and deploying your LangChain-based apps.  \n",
      "   ‚Ä¢ Key features  \n",
      "     ‚Äì Call logging & replay: track every prompt, response, tool action  \n",
      "     ‚Äì Prompt & model versioning: keep history of changes, A/B test prompts, providers  \n",
      "     ‚Äì Evaluation suites: write ‚Äúunit tests‚Äù for your chains, track success/failure metrics  \n",
      "     ‚Äì Visual tooling: chain/agent inspection, LangGraph integration, usage dashboards  \n",
      "     ‚Äì Deployments: host agents as APIs, set up auth, usage quotas, monitoring  \n",
      "   ‚Ä¢ Benefits  \n",
      "     ‚Äì Accelerates development by giving you end-to-end observability  \n",
      "     ‚Äì Helps maintain quality as your app and team scale\n",
      "\n",
      "In short:  \n",
      "‚Ä¢ LangChain = the local SDK for wiring together LLMs, tools, retrievers, memories.  \n",
      "‚Ä¢ LangGraph/langraph = a visualization/debug tool for seeing how your LangChain chains run.  \n",
      "‚Ä¢ LangSmith = the cloud service & UI for logging, testing, evaluating, and deploying those chains/agents in production.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Woguem Yen Fred! üëã\\n\\nIt's nice to meet you. What can I do for you today? üòä \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 18, 'total_tokens': 48, 'completion_time': 0.054545455, 'prompt_time': 0.002070255, 'queue_time': 0.159006732, 'total_time': 0.05661571}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--f51b2c8a-0feb-49f9-b205-5a9bd35e3cc8-0', usage_metadata={'input_tokens': 18, 'output_tokens': 30, 'total_tokens': 48})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model.invoke(\"Hi My name is Woguem Yen Fred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000027727628680>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000277274E3950>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000027727628680>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000277274E3950>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm familiar with LangSmith. It's an open-source platform designed to simplify the process of building and deploying large language models (LLMs). \n",
      "\n",
      "Here's a breakdown of what makes LangSmith noteworthy:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:** LangSmith boasts a web-based interface, making it accessible to a wider range of users, even those without extensive coding experience.\n",
      "* **Modular Design:** The platform is built with modularity in mind, allowing users to easily combine and customize different components to suit their specific needs.\n",
      "* **Fine-Tuning Capabilities:** LangSmith provides tools for fine-tuning pre-trained LLMs on custom datasets, enabling users to tailor models for specific tasks or domains.\n",
      "* **Experiment Tracking:** It includes features for tracking experiments and comparing different model configurations, facilitating the iterative development process.\n",
      "* **Community Driven:** Being open-source, LangSmith benefits from a vibrant community of developers and researchers who contribute to its growth and improvement.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Lower Barrier to Entry:** LangSmith democratizes access to LLM development by providing a more accessible and user-friendly platform.\n",
      "* **Increased Efficiency:** Its modular design and automation features streamline the development workflow, saving time and effort.\n",
      "* **Customization and Flexibility:** Users can fine-tune models and tailor them to their specific use cases, achieving better performance and accuracy.\n",
      "* **Collaboration and Innovation:** The open-source nature fosters collaboration and encourages the sharing of knowledge and best practices.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "LangSmith finds applications in various domains, including:\n",
      "\n",
      "* **Chatbots and Conversational AI:**\n",
      "\n",
      "Developing customized chatbots for customer service, education, or entertainment.\n",
      "* **Text Generation:**\n",
      "\n",
      "Creating compelling content, such as articles, stories, or marketing copy.\n",
      "* **Language Translation:**\n",
      "\n",
      "Fine-tuning models for accurate and efficient language translation.\n",
      "* **Code Generation:**\n",
      "\n",
      "Assisting developers in generating code snippets or completing code tasks.\n",
      "\n",
      "**Overall, LangSmith is a powerful and versatile platform that empowers individuals and organizations to leverage the potential of LLMs for a wide range of applications.**\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about LangSmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I can definitely tell you about Langsmith!\n",
      "\n",
      "Langsmith is a powerful and versatile open-source framework developed by the Hugging Face community. It's designed to simplify the process of fine-tuning and deploying language models, making it accessible to a wider range of users, from researchers to developers.\n",
      "\n",
      "Here are some key features and aspects of Langsmith:\n",
      "\n",
      "**1. Streamlined Fine-Tuning:**\n",
      "\n",
      "* **Simplified Workflow:** Langsmith offers a user-friendly interface and command-line tools that make fine-tuning language models significantly easier. It handles many of the technical complexities involved, allowing you to focus on your specific task.\n",
      "* **Data Management:** It provides tools for efficiently managing and preprocessing your training data, ensuring it's in the right format for fine-tuning.\n",
      "\n",
      "**2. Model Library and Hub:**\n",
      "\n",
      "* **Vast Model Selection:** Langsmith integrates with the Hugging Face Model Hub, giving you access to a massive collection of pre-trained language models. You can choose from various architectures and sizes to suit your needs.\n",
      "* **Model Sharing:** The framework facilitates sharing your fine-tuned models with the community, contributing to the collaborative nature of open-source development.\n",
      "\n",
      "**3. Deployment and Serving:**\n",
      "\n",
      "* **Easy Deployment:** Langsmith simplifies the process of deploying your fine-tuned models into production environments. It supports various deployment options, including local servers, cloud platforms, and web applications.\n",
      "* **Batch and Streaming Inference:** You can use Langsmith for both batch processing (generating responses for multiple inputs at once) and streaming inference (processing text input in real-time).\n",
      "\n",
      "**4. Community and Support:**\n",
      "\n",
      "* **Active Community:** Langsmith benefits from the active and supportive Hugging Face community. You can find documentation, tutorials, and assistance from other users and developers.\n",
      "\n",
      "**5. Open-Source Nature:**\n",
      "\n",
      "* **Transparency and Collaboration:** As an open-source project, Langsmith's code is freely available for anyone to inspect, modify, and contribute to. This fosters transparency and encourages collaboration in the AI community.\n",
      "\n",
      "**In essence, Langsmith empowers you to:**\n",
      "\n",
      "* **Easily fine-tune powerful language models for your specific tasks.**\n",
      "* **Leverage a vast library of pre-trained models.**\n",
      "* **Deploy your models effectively in various environments.**\n",
      "* **Contribute to the open-source AI ecosystem.**\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions or want to explore specific aspects of Langsmith in more detail!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI assistants.', 'features': ['Allows developers to create custom AI assistants tailored to specific needs.', 'Provides a user-friendly interface for interacting with and managing AI models.', 'Offers a range of pre-trained models for various tasks, such as text generation, summarization, and question answering.', 'Enables fine-tuning of existing models on custom datasets.', 'Supports multiple programming languages and frameworks.'], 'benefits': ['Increased efficiency and productivity by automating tasks.', 'Improved customer experience through personalized interactions.', 'Enhanced data analysis and insights.', 'Reduced development time and costs.'], 'website': 'https://www.langsmith.com/'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert football. Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert football. Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Cristiano Ronaldo', 'full_name': 'Cristiano Ronaldo dos Santos Aveiro', 'born': 'February 5, 1985', 'birthplace': 'Funchal, Madeira, Portugal', 'position': 'Forward', 'teams': ['Sporting CP', 'Manchester United', 'Real Madrid', 'Juventus', 'Manchester United', 'Al Nassr'], 'highlights': [\"5-time Ballon d'Or winner\", 'UEFA Champions League record goalscorer', 'All-time leading goalscorer in international football', 'Won numerous league titles in England, Spain, and Italy', 'Considered one of the greatest footballers of all time'], 'playing_style': 'Known for his incredible speed, dribbling skills, powerful shots, and aerial ability. A complete forward with an exceptional goal-scoring record.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Ronaldo?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb0abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n  <description>Langsmith is an open-source platform developed by Google DeepMind that streamlines the development and deployment of large language models (LLMs). It provides a comprehensive suite of tools and infrastructure to manage the entire LLM lifecycle, from training and evaluation to serving and monitoring.</description>\\n  <features>\\n    <feature>Modular design for flexibility and extensibility</feature>\\n    <feature>Support for various hardware architectures, including CPUs, GPUs, and TPUs</feature>\\n    <feature>Integrated tools for model training, evaluation, and deployment</feature>\\n    <feature>Scalable infrastructure for handling large-scale LLM workloads</feature>\\n    <feature>Open-source nature fosters community contributions and collaboration</feature>\\n  </features>\\n</response>\\n``` \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 195, 'total_tokens': 373, 'completion_time': 0.323636364, 'prompt_time': 0.007833753, 'queue_time': 0.161956616, 'total_time': 0.331470117}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--38a39738-3991-4bcb-911b-640113cd563d-0' usage_metadata={'input_tokens': 195, 'output_tokens': 178, 'total_tokens': 373}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response><answer>LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the process of building with LLMs by providing tools for:\n",
      "\n",
      "- **Chain together LLMs with other tools**: Integrate LLMs with databases, APIs, and other services to create more powerful applications.\n",
      "- **Manage LLMs effectively**: Handle prompt engineering, memory management, and other aspects of LLM interaction.\n",
      "- **Build custom LLM applications**: Create your own unique applications using pre-built components or from scratch. </answer></response>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchligne': 'Because it lost its bearings.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchligne: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"Sure! Here is a story about science: Once upon a time, there was a young scientist named Marie Curie. She was fascinated by the mysterious properties of a new element called radium. Through her tireless experiments and dedication to her research, Marie discovered that radium emitted powerful radiation that could be used to treat cancer. Her groundbreaking work in the field of radioactivity earned her two Nobel Prizes and revolutionized the field of medicine. Marie Curie's story is a testament to the power of curiosity, perseverance, and the impact that science can have on the world.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a story about science.\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Splash</movie>\n",
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>The Da Vinci Code</movie>\n",
      "<movie>Toy Story (franchise)</movie>\n",
      "<movie>Captain Phillips</movie>\n",
      "<movie>Sully</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle find its way home?\", punchline='Because it lost its bearings!')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=1.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecbd46c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product_Name': 'Topicrem',\n",
       " 'Product_details_price': 'Topicrem products vary in price, ranging from $10-$30 USD depending on the size and type of product.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Product(BaseModel):\n",
    "    Product_Name: str = Field(description=\"give the name of the product\")\n",
    "    Product_details_price: str = Field(description=\"give the details of the product price in USD, mentione the currency (e.g. '15 USD').\")\n",
    "\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "escaped_format = format_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "# Query for product.\n",
    "product_query = \"Tell me about topicrem.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",f\"You are an expert product researcher. Provide me answer based on the question. Please answer the user's query using the format below:\\n{escaped_format}\"),\n",
    "        (\"user\",\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": product_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5b001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtual_lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
